# -*- coding: utf-8 -*-
"""YOLO11 Tutorial

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb

<div align="center">
  <a href="https://ultralytics.com/yolo" target="_blank">
    <img width="1024" src="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png">
  </a>

  [‰∏≠Êñá](https://docs.ultralytics.com/zh/) | [ÌïúÍµ≠Ïñ¥](https://docs.ultralytics.com/ko/) | [Êó•Êú¨Ë™û](https://docs.ultralytics.com/ja/) | [–†—É—Å—Å–∫–∏–π](https://docs.ultralytics.com/ru/) | [Deutsch](https://docs.ultralytics.com/de/) | [Fran√ßais](https://docs.ultralytics.com/fr/) | [Espa√±ol](https://docs.ultralytics.com/es/) | [Portugu√™s](https://docs.ultralytics.com/pt/) | [T√ºrk√ße](https://docs.ultralytics.com/tr/) | [Ti·∫øng Vi·ªát](https://docs.ultralytics.com/vi/) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](https://docs.ultralytics.com/ar/)

  <a href="https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml"><img src="https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg" alt="Ultralytics CI"></a>
  <a href="https://console.paperspace.com/github/ultralytics/ultralytics"><img src="https://assets.paperspace.io/img/gradient-badge.svg" alt="Run on Gradient"/></a>
  <a href="https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>
  <a href="https://www.kaggle.com/models/ultralytics/yolo11"><img src="https://kaggle.com/static/images/open-in-kaggle.svg" alt="Open In Kaggle"></a>

  <a href="https://ultralytics.com/discord"><img alt="Discord" src="https://img.shields.io/discord/1089800235347353640?logo=discord&logoColor=white&label=Discord&color=blue"></a>
  <a href="https://community.ultralytics.com"><img alt="Ultralytics Forums" src="https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&logo=discourse&label=Forums&color=blue"></a>
  <a href="https://reddit.com/r/ultralytics"><img alt="Ultralytics Reddit" src="https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&logo=reddit&logoColor=white&label=Reddit&color=blue"></a>
</div>

This **Ultralytics Colab Notebook** is the easiest way to get started with [YOLO models](https://www.ultralytics.com/yolo)‚Äîno installation needed. Built by [Ultralytics](https://www.ultralytics.com/), the creators of YOLO, this notebook walks you through running **state-of-the-art** models directly in your browser.

Ultralytics models are constantly updated for performance and flexibility. They're **fast**, **accurate**, and **easy to use**, and they excel at [object detection](https://docs.ultralytics.com/tasks/detect/), [tracking](https://docs.ultralytics.com/modes/track/), [instance segmentation](https://docs.ultralytics.com/tasks/segment/), [image classification](https://docs.ultralytics.com/tasks/classify/), and [pose estimation](https://docs.ultralytics.com/tasks/pose/).

Find detailed documentation in the [Ultralytics Docs](https://docs.ultralytics.com/). Get support via [GitHub Issues](https://github.com/ultralytics/ultralytics/issues/new/choose). Join discussions on [Discord](https://discord.com/invite/ultralytics), [Reddit](https://www.reddit.com/r/ultralytics/), and the [Ultralytics Community Forums](https://community.ultralytics.com/)!

Request an Enterprise License for commercial use at [Ultralytics Licensing](https://www.ultralytics.com/license).

<br>
<div>
  <a href="https://www.youtube.com/watch?v=ZN3nRZT7b24" target="_blank">
    <img src="https://img.youtube.com/vi/ZN3nRZT7b24/maxresdefault.jpg" alt="Ultralytics Video" width="640" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);">
  </a>

  <p style="font-size: 16px; font-family: Arial, sans-serif; color: #555;">
    <strong>Watch: </strong> How to Train
    <a href="https://github.com/ultralytics/ultralytics">Ultralytics</a>
    <a href="https://docs.ultralytics.com/models/yolo11/">YOLO11</a> Model on Custom Dataset using Google Colab Notebook üöÄ
  </p>
</div>

# Setup

pip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) and check software and hardware.

[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&logoColor=white)](https://pypi.org/project/ultralytics/) [![Downloads](https://static.pepy.tech/badge/ultralytics)](https://www.pepy.tech/projects/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&logoColor=gold)](https://pypi.org/project/ultralytics/)
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install ultralytics
import ultralytics
ultralytics.checks()

!wget https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-pose.pt

import os
import cv2
import json
import numpy as np
from ultralytics import YOLO

# Ensure output directory exists
output_dir = "/content/output"
os.makedirs(output_dir, exist_ok=True)

# Define the JSON file path
json_path = os.path.join(output_dir, "pose.json")

# Load YOLO Pose Model
model = YOLO("yolo11m-pose.pt")

# Open video source (replace with 0 for webcam)
cap = cv2.VideoCapture("/content/ku.mp4")

# Get video properties
fps = cap.get(cv2.CAP_PROP_FPS)
width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# Define the codec and create VideoWriter object
output_video_path = os.path.join(output_dir, "annotated_output.mp4")
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

pose_data = []
frame_count = 0

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    results = model(frame)
    annotated_frame = results[0].plot()

    for result in results:
        keypoints = result.keypoints.xy.cpu().numpy() if result.keypoints is not None else np.array([])
        bbox = result.boxes.xyxy.cpu().numpy() if result.boxes is not None else None

        if keypoints.size > 0:
            frame_data = {
                "frame_id": frame_count,
                "keypoints": keypoints.tolist(),
                "bounding_box": bbox.tolist() if bbox is not None else None
            }

            pose_data.append(frame_data)

            # Save to JSON
            try:
                with open(json_path, "w") as f:
                    json.dump(pose_data, f, indent=4)
                print(f"‚úÖ Saved frame {frame_count} data to {json_path}")
            except Exception as e:
                print(f"‚ùå ERROR saving JSON: {e}")

    # Write the annotated frame to video
    out.write(annotated_frame)

    frame_count += 1

# Release resources
cap.release()
out.release()

print(f"‚úÖ Annotated video saved at: {output_video_path}")
print(f"‚úÖ Final JSON saved at: {json_path}")

!wget https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-pose.pt

import os
import cv2
import json
import numpy as np
from ultralytics import YOLO

# Ensure output directory exists
output_dir = "output"
os.makedirs(output_dir, exist_ok=True)

# Load the YOLOv11x-pose model (highest accuracy)
model = YOLO("yolo11x-pose.pt")  # ensure this file is available

# Open video source (replace with 0 for webcam or a file path)
cap = cv2.VideoCapture("/content/ku.mp4")
if not cap.isOpened():
    raise IOError("Cannot open video file")

# Video properties and output setup (HD 1280x720)
fps    = cap.get(cv2.CAP_PROP_FPS)
width  = 1280
height = 720
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter("output/annotated_output.mp4", fourcc, fps, (width, height))

pose_data = []
frame_count = 0

while True:
    ret, frame = cap.read()
    if not ret:
        break  # end of video

    # Resize frame to 1280x720
    frame = cv2.resize(frame, (width, height))

    # Run pose estimation with tracking enabled
    results = model.track(frame, task="pose", persist=True)
    res = results[0]  # result for the current frame

    # Draw skeletons/keypoints on the frame
    annotated_frame = res.plot()

    # If there are detections, extract their data
    if res.boxes is not None and len(res.boxes.xyxy):
        # Get tracking IDs (one per detected person)
        if res.boxes.id is not None:
            track_ids = res.boxes.id.int().cpu().numpy().tolist()
        else:
            track_ids = [None] * len(res.boxes.xyxy)

        # Iterate over each detected person i
        for i, bbox in enumerate(res.boxes.xyxy):
            person_id = int(track_ids[i]) if track_ids[i] is not None else None
            # 17 x [x, y] keypoint coordinates
            keypoints = res.keypoints.xy[i].cpu().numpy().tolist()
            # Bounding box [x_min, y_min, x_max, y_max]
            bbox_xyxy = bbox.cpu().numpy().tolist()

            frame_data = {
                "frame_id": frame_count,
                "person_id": person_id,
                "keypoints": keypoints,
                "bounding_box": bbox_xyxy
            }
            pose_data.append(frame_data)

    # Write the annotated frame to output video
    out.write(annotated_frame)
    frame_count += 1

# Release resources
cap.release()
out.release()

# Save all pose data to JSON file
with open("output/pose_data.json", "w") as f:
    json.dump(pose_data, f, indent=4)

print("‚úÖ Annotated video saved as output/annotated_output.mp4")
print("‚úÖ Pose data saved as output/pose_data.json")



import os
import cv2
import json
import numpy as np
from ultralytics import YOLO

# Ensure output directory exists
output_dir = "output"
os.makedirs(output_dir, exist_ok=True)

# Load the YOLOv11x-pose model (highest accuracy)
model = YOLO("yolo11x-pose.pt")  # ensure this file is available

# Open video source (replace with 0 for webcam or a file path)
cap = cv2.VideoCapture("input_video.mp4")
if not cap.isOpened():
    raise IOError("Cannot open video file")

# Video properties and output setup (HD 1280x720)
fps    = cap.get(cv2.CAP_PROP_FPS)
width  = 1280
height = 720
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter("output/annotated_output.mp4", fourcc, fps, (width, height))

pose_data = []
frame_count = 0

while True:
    ret, frame = cap.read()
    if not ret:
        break  # end of video

    # Resize frame to 1280x720
    frame = cv2.resize(frame, (width, height))

    # Run pose estimation with tracking enabled
    results = model.track(frame, task="pose", persist=True)
    res = results[0]  # result for the current frame

    # Draw skeletons/keypoints on the frame
    annotated_frame = res.plot()

    # If there are detections, extract their data
    if res.boxes is not None and len(res.boxes.xyxy):
        # Get tracking IDs (one per detected person)
        if res.boxes.id is not None:
            track_ids = res.boxes.id.int().cpu().numpy().tolist()
        else:
            track_ids = [None] * len(res.boxes.xyxy)

        # Iterate over each detected person i
        for i, bbox in enumerate(res.boxes.xyxy):
            person_id = int(track_ids[i]) if track_ids[i] is not None else None
            # 17 x [x, y] keypoint coordinates
            keypoints = res.keypoints.xy[i].cpu().numpy().tolist()
            # Bounding box [x_min, y_min, x_max, y_max]
            bbox_xyxy = bbox.cpu().numpy().tolist()

            frame_data = {
                "frame_id": frame_count,
                "person_id": person_id,
                "keypoints": keypoints,
                "bounding_box": bbox_xyxy
            }
            pose_data.append(frame_data)

    # Write the annotated frame to output video
    out.write(annotated_frame)
    frame_count += 1

# Release resources
cap.release()
out.release()

# Save all pose data to JSON file
with open("output/pose_data.json", "w") as f:
    json.dump(pose_data, f, indent=4)

print("‚úÖ Annotated video saved as output/annotated_output.mp4")
print("‚úÖ Pose data saved as output/pose_data.json")

with velocity

from ultralytics import YOLO
import cv2, json
import numpy as np
import os

# Load pretrained YOLOv11 pose model
model = YOLO("yolo11x-pose.pt")  # official large pose model:contentReference[oaicite:3]{index=3}

cap = cv2.VideoCapture("input_video.mp4")
fps = cap.get(cv2.CAP_PROP_FPS)
width, height = 1280, 720
out = cv2.VideoWriter("output/annotated_output.mp4", cv2.VideoWriter_fourcc(*'mp4v'),
                      fps, (width, height))
pose_data = []
frame_count = 0

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Resize for consistent input size
    frame = cv2.resize(frame, (width, height))

    # Run pose estimation with tracking to keep IDs across frames:contentReference[oaicite:4]{index=4}
    results = model.track(frame, task="pose", persist=True)
    res = results[0]  # current frame result
    annotated_frame = res.plot()  # draw skeleton

    # Compute timestamp (in seconds) for this frame
    timestamp = frame_count / fps

    # If at least one person detected, extract data
    if res.boxes is not None and len(res.boxes.xyxy):
        # Get tracking IDs (or None if disabled)
        track_ids = res.boxes.id.int().cpu().numpy().tolist() \
                    if res.boxes.id is not None else [None]*len(res.boxes.xyxy)

        # Iterate detected persons
        for i, bbox in enumerate(res.boxes.xyxy):
            person_id = int(track_ids[i]) if track_ids[i] is not None else None
            # Extract 17 keypoints [x, y] for person i
            keypoints = res.keypoints.xy[i].cpu().numpy().tolist()
            # Convert bbox tensor to list
            bbox_xyxy = bbox.cpu().numpy().tolist()

            # -- Compute limb vectors and magnitudes for arms and legs --
            # (Define COCO indices: LShoulder=5, LElbow=7, LWrist=9; RShoulder=6, RElbow=8, RWrist=10;
            #                       LHip=11, LKnee=13, LAnkle=15;  RHip=12, RKnee=14, RAnkle=16)
            vec_data = {}
            pts = np.array(keypoints)  # 17x2 array of (x,y)
            # Left arm: shoulder->elbow and elbow->wrist
            L_shldr, L_elbow, L_wrist = pts[5], pts[7], pts[9]
            v1 = L_elbow - L_shldr;   v2 = L_wrist - L_elbow
            # Right arm
            R_shldr, R_elbow, R_wrist = pts[6], pts[8], pts[10]
            v3 = R_elbow - R_shldr;   v4 = R_wrist - R_elbow
            # Left leg
            L_hip, L_knee, L_ankle = pts[11], pts[13], pts[15]
            v5 = L_knee - L_hip;      v6 = L_ankle - L_knee
            # Right leg
            R_hip, R_knee, R_ankle = pts[12], pts[14], pts[16]
            v7 = R_knee - R_hip;      v8 = R_ankle - R_knee

            # Store each vector's dx, dy, and magnitude
            vecs = [("L_upper_arm", v1), ("L_lower_arm", v2),
                    ("R_upper_arm", v3), ("R_lower_arm", v4),
                    ("L_thigh", v5), ("L_shin", v6),
                    ("R_thigh", v7), ("R_shin", v8)]
            for name, vec in vecs:
                dx, dy = float(vec[0]), float(vec[1])
                mag = (dx**2 + dy**2)**0.5
                vec_data[name] = {"dx": dx, "dy": dy, "magnitude": mag}

            # Optionally, compute resultant vectors (shoulder->wrist, hip->ankle)
            res_vecs = {
                "L_arm_result": (L_wrist - L_shldr),
                "R_arm_result": (R_wrist - R_shldr),
                "L_leg_result": (L_ankle - L_hip),
                "R_leg_result": (R_ankle - R_hip)
            }
            for name, vec in res_vecs.items():
                dx, dy = float(vec[0]), float(vec[1])
                mag = (dx**2 + dy**2)**0.5
                vec_data[name] = {"dx": dx, "dy": dy, "magnitude": mag}

            # Record frame data including vectors
            frame_data = {
                "frame_id": frame_count,
                "timestamp": timestamp,
                "person_id": person_id,
                "keypoints": keypoints,
                "bounding_box": bbox_xyxy,
                "limb_vectors": vec_data
            }
            pose_data.append(frame_data)

    out.write(annotated_frame)
    frame_count += 1

cap.release()
out.release()
with open("output/pose_data.json", "w") as f:
    json.dump(pose_data, f, indent=4)
print("‚úÖ Annotated video and pose data saved.")

"""# 1. Predict

YOLO11 may be used directly in the Command Line Interface (CLI) with a `yolo` command for a variety of tasks and modes and accepts additional arguments, i.e. `imgsz=640`. See a full list of available `yolo` [arguments](https://docs.ultralytics.com/usage/cfg/) and other details in the [YOLO11 Predict Docs](https://docs.ultralytics.com/modes/train/).

"""

# Run inference on an image with YOLO11n
!yolo predict model=yolo11n.pt source='https://ultralytics.com/images/zidane.jpg'

"""&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<img align="left" src="https://user-images.githubusercontent.com/26833433/212889447-69e5bdf1-5800-4e29-835e-2ed2336dede2.jpg" width="600">

# 2. Val
Validate a model's accuracy on the [COCO](https://docs.ultralytics.com/datasets/detect/coco/) dataset's `val` or `test` splits. The latest YOLO11 [models](https://github.com/ultralytics/ultralytics#models) are downloaded automatically the first time they are used. See [YOLO11 Val Docs](https://docs.ultralytics.com/modes/val/) for more information.
"""

# Download COCO val
import torch
torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)
!unzip -q tmp.zip -d datasets && rm tmp.zip  # unzip

# Validate YOLO11n on COCO8 val
!yolo val model=yolo11n.pt data=coco8.yaml

"""# 3. Train

<p align=""><a href="https://ultralytics.com/hub"><img width="1000" src="https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png"/></a></p>

Train YOLO11 on [Detect](https://docs.ultralytics.com/tasks/detect/), [Segment](https://docs.ultralytics.com/tasks/segment/), [Classify](https://docs.ultralytics.com/tasks/classify/) and [Pose](https://docs.ultralytics.com/tasks/pose/) datasets. See [YOLO11 Train Docs](https://docs.ultralytics.com/modes/train/) for more information.
"""

# Commented out IPython magic to ensure Python compatibility.
#@title Select YOLO11 üöÄ logger {run: 'auto'}
logger = 'TensorBoard' #@param ['TensorBoard', 'Weights & Biases']

if logger == 'TensorBoard':
  !yolo settings tensorboard=True
#   %load_ext tensorboard
#   %tensorboard --logdir .
elif logger == 'Weights & Biases':
  !yolo settings wandb=True

# Train YOLO11n on COCO8 for 3 epochs
!yolo train model=yolo11n.pt data=coco8.yaml epochs=3 imgsz=640

"""# 4. Export

Export a YOLO model to any supported format below with the `format` argument, i.e. `format=onnx`. See [Export Docs](https://docs.ultralytics.com/modes/export/) for more information.

- üí° ProTip: Export to [ONNX](https://docs.ultralytics.com/integrations/onnx/) or [OpenVINO](https://docs.ultralytics.com/integrations/openvino/) for up to 3x CPU speedup.
- üí° ProTip: Export to [TensorRT](https://docs.ultralytics.com/integrations/tensorrt/) for up to 5x GPU speedup.

| Format | `format` Argument | Model | Metadata | Arguments |
|--------|-----------------|-------|----------|------------|
| [PyTorch](https://pytorch.org/) | - | `yolo11n.pt` | ‚úÖ | - |
| [TorchScript](https://docs.ultralytics.com/integrations/torchscript) | `torchscript` | `yolo11n.torchscript` | ‚úÖ | `imgsz`, `batch`, `optimize`, `half`, `nms`, `device` |
| [ONNX](https://docs.ultralytics.com/integrations/onnx) | `onnx` | `yolo11n.onnx` | ‚úÖ | `imgsz`, `batch`, `dynamic`, `half`, `opset`, `simplify`, `nms`, `device` |
| [OpenVINO](https://docs.ultralytics.com/integrations/openvino) | `openvino` | `yolo11n_openvino_model/` | ‚úÖ | `imgsz`, `batch`, `dynamic`, `half`, `int8`, `nms`, `fraction`, `device`, `data` |
| [TensorRT](https://docs.ultralytics.com/integrations/tensorrt) | `engine` | `yolo11n.engine` | ‚úÖ | `imgsz`, `batch`, `dynamic`, `half`, `int8`, `simplify`, `nms`, `fraction`, `device`, `data`, `workspace` |
| [CoreML](https://docs.ultralytics.com/integrations/coreml) | `coreml` | `yolo11n.mlpackage` | ‚úÖ | `imgsz`, `batch`, `half`, `int8`, `nms`, `device` |
| [TF SavedModel](https://docs.ultralytics.com/integrations/tf-savedmodel) | `saved_model` | `yolo11n_saved_model/` | ‚úÖ | `imgsz`, `batch`, `int8`, `keras`, `nms`, `device` |
| [TF GraphDef](https://docs.ultralytics.com/integrations/tf-graphdef) | `pb` | `yolo11n.pb` | ‚ùå | `imgsz`, `batch`, `device` |
| [TF Lite](https://docs.ultralytics.com/integrations/tflite) | `tflite` | `yolo11n.tflite` | ‚úÖ | `imgsz`, `batch`, `half`, `int8`, `nms`, `fraction`, `device`, `data` |
| [TF Edge TPU](https://docs.ultralytics.com/integrations/edge-tpu) | `edgetpu` | `yolo11n_edgetpu.tflite` | ‚úÖ | `imgsz`, `device` |
| [TF.js](https://docs.ultralytics.com/integrations/tfjs) | `tfjs` | `yolo11n_web_model/` | ‚úÖ | `imgsz`, `batch`, `half`, `int8`, `nms`, `device` |
| [PaddlePaddle](https://docs.ultralytics.com/integrations/paddlepaddle) | `paddle` | `yolo11n_paddle_model/` | ‚úÖ | `imgsz`, `batch`, `device` |
| [MNN](https://docs.ultralytics.com/integrations/mnn) | `mnn` | `yolo11n.mnn` | ‚úÖ | `imgsz`, `batch`, `half`, `int8`, `device` |
| [NCNN](https://docs.ultralytics.com/integrations/ncnn) | `ncnn` | `yolo11n_ncnn_model/` | ‚úÖ | `imgsz`, `batch`, `half`, `device` |
| [IMX500](https://docs.ultralytics.com/integrations/sony-imx500) | `imx` | `yolo11n_imx_model/` | ‚úÖ | `imgsz`, `int8`, `fraction`, `device`, `data` |
| [RKNN](https://docs.ultralytics.com/integrations/rockchip-rknn) | `rknn` | `yolo11n_rknn_model/` | ‚úÖ | `imgsz`, `batch`, `name`, `device` |
"""

!yolo export model=yolo11n.pt format=torchscript

"""# 5. Python Usage

YOLO11 was reimagined using Python-first principles for the most seamless Python YOLO experience yet. YOLO11 models can be loaded from a trained checkpoint or created from scratch. Then methods are used to train, val, predict, and export the model. See detailed Python usage examples in the [YOLO11 Python Docs](https://docs.ultralytics.com/usage/python/).
"""

from ultralytics import YOLO

# Load a model
model = YOLO('yolo11n.yaml')  # build a new model from scratch
model = YOLO('yolo11n.pt')  # load a pretrained model (recommended for training)

# Use the model
results = model.train(data='coco8.yaml', epochs=3)  # train the model
results = model.val()  # evaluate model performance on the validation set
results = model('https://ultralytics.com/images/bus.jpg')  # predict on an image
results = model.export(format='onnx')  # export the model to ONNX format

"""# 6. Tasks

YOLO11 can train, val, predict and export models for the most common tasks in vision AI: [Detect](https://docs.ultralytics.com/tasks/detect/), [Segment](https://docs.ultralytics.com/tasks/segment/), [Classify](https://docs.ultralytics.com/tasks/classify/) and [Pose](https://docs.ultralytics.com/tasks/pose/). See [YOLO11 Tasks Docs](https://docs.ultralytics.com/tasks/) for more information.

<br><img width="1024" src="https://raw.githubusercontent.com/ultralytics/assets/main/im/banner-tasks.png">

## 1. Detection

YOLO11 _detection_ models have no suffix and are the default YOLO11 models, i.e. `yolo11n.pt` and are pretrained on COCO. See [Detection Docs](https://docs.ultralytics.com/tasks/detect/) for full details.
"""

# Load YOLO11n, train it on COCO128 for 3 epochs and predict an image with it
from ultralytics import YOLO

model = YOLO('yolo11n.pt')  # load a pretrained YOLO detection model
model.train(data='coco8.yaml', epochs=3)  # train the model
model('https://ultralytics.com/images/bus.jpg')  # predict on an image

"""## 2. Segmentation

YOLO11 _segmentation_ models use the `-seg` suffix, i.e. `yolo11n-seg.pt` and are pretrained on COCO. See [Segmentation Docs](https://docs.ultralytics.com/tasks/segment/) for full details.

"""

# Load YOLO11n-seg, train it on COCO128-seg for 3 epochs and predict an image with it
from ultralytics import YOLO

model = YOLO('yolo11n-seg.pt')  # load a pretrained YOLO segmentation model
model.train(data='coco8-seg.yaml', epochs=3)  # train the model
model('https://ultralytics.com/images/bus.jpg')  # predict on an image

"""## 3. Classification

YOLO11 _classification_ models use the `-cls` suffix, i.e. `yolo11n-cls.pt` and are pretrained on ImageNet. See [Classification Docs](https://docs.ultralytics.com/tasks/classify/) for full details.

"""

# Load YOLO11n-cls, train it on mnist160 for 3 epochs and predict an image with it
from ultralytics import YOLO

model = YOLO('yolo11n-cls.pt')  # load a pretrained YOLO classification model
model.train(data='mnist160', epochs=3)  # train the model
model('https://ultralytics.com/images/bus.jpg')  # predict on an image

"""## 4. Pose

YOLO11 _pose_ models use the `-pose` suffix, i.e. `yolo11n-pose.pt` and are pretrained on COCO Keypoints. See [Pose Docs](https://docs.ultralytics.com/tasks/pose/) for full details.
"""

# Load YOLO11n-pose, train it on COCO8-pose for 3 epochs and predict an image with it
from ultralytics import YOLO

model = YOLO('yolo11n-pose.pt')  # load a pretrained YOLO pose model
model.train(data='coco8-pose.yaml', epochs=3)  # train the model
model('https://ultralytics.com/images/bus.jpg')  # predict on an image

"""## 4. Oriented Bounding Boxes (OBB)

YOLO11 _OBB_ models use the `-obb` suffix, i.e. `yolo11n-obb.pt` and are pretrained on the DOTA dataset. See [OBB Docs](https://docs.ultralytics.com/tasks/obb/) for full details.
"""

# Load YOLO11n-obb, train it on DOTA8 for 3 epochs and predict an image with it
from ultralytics import YOLO

model = YOLO('yolo11n-obb.pt')  # load a pretrained YOLO OBB model
model.train(data='dota8.yaml', epochs=3)  # train the model
model('https://ultralytics.com/images/boats.jpg')  # predict on an image

"""# Appendix

Additional content below.
"""

# Pip install from source
!pip install git+https://github.com/ultralytics/ultralytics@main

# Commented out IPython magic to ensure Python compatibility.
# Git clone and run tests on 'main' branch
!git clone https://github.com/ultralytics/ultralytics -b main
# %pip install -qe ultralytics

# Run tests (Git clone only)
!pytest ultralytics/tests

# Validate multiple models
for x in 'nsmlx':
  !yolo val model=yolo11{x}.pt data=coco.yaml